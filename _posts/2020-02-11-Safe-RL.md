---
layout:     post
title:      "关于Safe DL的一些论文及想法"
subtitle:   "算是对自己挣扎半年多的一个梳理吧。"
date:       2020-02-11
author:     "JohnReese"
header-img: "img/2020-2-11/stone.jpg"
mathjax: True
tags:
    - RL
    - ML
---

更早的一段时间关注的更多是有关于`DNN`的形式化验证。再具体一些，就是对抗样本的检测。这最早应该是源于[1]等人在2013年的发现，到现在为止，这篇论文在谷歌学术上已经有近4000的引用量了。

了解`DNN`的朋友应该知道`DNN`是一个类似于指数级复杂度（我不知道这样表述是否准确）的函数。而对抗样本所表述的问题是对于这个函数输入的细微干扰可以导致函数输出的急剧变化。从数学的角度看这没什么问题，但联系到`DNN`的实际应用，就意味着一张有细微噪声的图片会带来意想不到的结果。

因此，`researchers`就尝试使用各种方法来证明是否有对抗样本可以“愚弄”训练完成的`DNN`[2]。就我现在还记得的一些方法包括线性规划，抽象解释，区间约束等。其中由此还衍生出许多相关的方向，比如如何最有效地找到`DNN`的对抗样本；如何解释`DNN`是如何做决策的；如何增强`DNN`在这方面的鲁棒性等等问题。

这段时间给我印象比较深的是苏黎世联邦理工学院的[SafeAI实验室](http://safeai.ethz.ch/)，真的是tql——几年都能保持这个方向的顶会论文产出。

而看得多了，我渐渐认为这几乎就是一个纯数学问题：如何以我们现在可以接受的时间复杂度内不断地提高验证的精度？此时我内心已经气馁了，因为别人的点子永远是你TM之前完全没有听过的名词，于是我觉得在这个方向发论文太难了。

当然，我也知道发论文不需要有完全原创的内容，只需要有一定的改进，你的工作就会得到认可。但很遗憾，这个过程中我几乎没有任何想法。现在基本转到`Safe RL`上了，感觉以前的时间都浪费了，唉，这就是无能狂怒吧。

不过个人感觉强化学习确实比`DNN`有趣一点，也更加“新”一点。通俗地讲，`RL`就是训练一个`Agent`。比如需要完成在一个网格世界中从起点走向终点的任务，这个`Agent`就会不断地尝试四个方向地行走。当然，有时它会走到`Trap`(人为设定)中而不能完成任务，那么它就会重新来过。每一次的尝试都会帮助它调整自己的策略，简直就像小孩子学习做一件事一样。

目前关于`Safe RL`我觉得主要有两块内容。[3]分类得更加细致，但我觉得主要就是训练时的安全性和训练后的安全性。而训练时的安全性就是`Safe Exploration`。回顾上一节所说的`Agent`可能会走到Trap里，而对于其他任务，比如训练`Agent`控制直升飞机，可能Trap就意味着飞机坠落，由此而导致的问题就是训练的代价过大，以至于使用现实环境训练不太可行。而`Safe Exploration`在2015年有一篇综述写得不错[4]，主要的方法大致有：
1. Worst Case Criterion: 最坏情况的判定
2. Risk-Sensitive Criterion： 是否危险的判定
3. Providing Initial Knowledge: 训练前提供先验知识
4. Deriving a Policy from a Finite Set of Demonstrations：从演示中学习
5. Using Teacher Advice：人工介入训练过程

看了一些论文，我觉得`Agent`一定要有先验知识才能达到`Safe Exploration`。为啥呢？因为我联系到人学车时的一些想法：不管刚开始对车有多无知，我们一定会低速行驶；在前面有障碍物的时候采取转弯或停车。这些细节没人需要去教你，因为我们都知道。那我们知道的原因就是我们的头脑中有碰撞及其后果的概念，简直就类似于`Agent`会遇到的下一个`State`及其Reward。然后我又想如何尽量简洁地将先验知识传递给`Agent`，我觉得答案是`Reward`。因为这是在RL中极少由人为设定的变量（其实应该是函数）。   

于是我就开始找有没有和RL中`Reward`相关的论文呢？找了半天，就在我以为这是一个新的切入点时，突然发现了两篇Andrew Ng的开山之作，实在是tql，我还以为吴恩达只是DL方面的高手，真的是只能佩服不已。但我的心也瞬间就凉了。

第一篇[5]关于`Reward Shaping`，发表时间是1999年，距今已有1000多的引用量。论文直接提出一个问题：我们究竟对于`Reward Function`的定义有多大的自由度，使得习得的最优策略不会改变？（我能说我也想得差不多嘛，就是晚了20年。。。）接着提到了一个模拟自行车的系统，为了加快训练，只要自行车接近目标系统就会给予正向奖励，结果你猜怎么着，自行车在出发点绕起了圈圈。因为这样的行为不会受到负奖励。也就是说，设计有问题的`Reward Function`会导致`Agent`习得非最优策略。

那么如何设计`Reward Function`呢？Andrew他们实在太强了：首先他们认为无需借助其它先验知识；其次自行车这个实验提供了一个很好的启发，即`Agent`之所以习得如下策略是因为出现了循环，其中`F`表示奖励函数：

$$
S_1 \rightarrow S_2 ... \rightarrow S_n \rightarrow S_1 ...\\
F(S_1, a_1, S_2) + F(S_2, a_2, S_3) + ... + F(S_n, a_n, S_1) > 0
$$

那么我们在原来的`Reward Function`上添加类似于$F(s, a, s^{'}) = \Phi(s^{'}) - \Phi(s)$的函数形式不就可以抵消这样的正向推动吗？而且他们提到在没有进一步的先验知识下，这似乎是`唯一`的构造方法。而且实验结果在我看来也是非常优秀。但是并没有完全解决`Reward Function`的构造问题，因为$\Phi(s)$也是需要人为设计的，可能就是在原来的基础上效果更好。

第二篇[6]也是关于`Reward Function`的构造问题，发表时间为2000年，至今引用量近2000。主要的内容是`Inverse Reinforcement Learning`，顾名思义，反过来学习，即一开始没有`Reward Function`，而是从专家的演示中习得`Reward Function`。作者分别对有限状态，无限状态（连续状态）和样本迹三种情况依次建立目标函数及约束，类似于线性规划的求解。

至此，我就了解了目前至少有`Reward Shaping`和`IRL`两种建立`Reward Function`的方法。当然，它们也不完美，但是至少说明researchers不是“戆笃”——你想到完全创新的点子之后只管去找论文就对了，找不到可能只是量不够或者方式不对。。。

其实本质上`Reward Function`的最优解应该与`Value Function`有关，但是训练的目的就是获得`Value Function`，所以这就是一个矛盾。但就我这几个月的直觉来看，`Reward Function`对于训练的安全性和效率应该是异常重要的。

至于其它方法，我都还没来得及深入，就好像只有退潮了,才知道谁在裸泳——只有自我摸索新领域的时候才能看出来谁是菜鸡。

至于对于训练完的`Agent`进行形式化验证的论文就比较少了。我觉得原因就是目前的主流做法是依靠DNN来近似拟合`Value Function`。所以训练完成的结果就是一个`DNN`，哈哈，TM又回到DNN上了，就NM离谱。

目前，形式化加`RL`的论文我就看到过一篇[7]，还是导师帮我找的。至于具体内容真不是我吹，这哥们想的我也想到过，就是将`DNN`联系到`Agent`所处的上下文中。但是他们包装得很好，引入了`Safety`和`Liveness`。而且我觉得这篇论文的实现方式讲得不够透彻（当然不排除我没看懂）。因为安全性（`Safety`）说到底就是在某些状态不能执行某些动作，比如前方有障碍（`State`）不能加速（`Action`）。那么`State`就是DNN的某个输入或者一些输入，看看输出就知道会不会执行加速这个行为了。因此我觉得此类工作没什么价值（真香警告！）。

好了，到此总结下我的半年“裸泳生涯”吧。之后的重心肯定不在DNN的形式化验证上了，已经落后太多了。至于`Safe Exploration`肯定比形式化的想法多，但是方法一多你就需要逐个调研。但说到底，形式化验证才是初衷，奈何`Deep Learning`掀起的波澜实在太大。说出来我都不信，我是不想搞`Deep Learning`才来形式化的啊。。。而且咱们只能搞搞设计工程什么的，数学什么的咱实在是不会啊。。。

研究生的论文对于菜鸡来说岂一个惨字了得啊。。。

但我也不能忘记自己读研的初心：只是想更加了解这个世界一点，人生对我来说在大多数情况下应该还很长，不想早早地丢掉自己的求知欲。

## References
1. Szegedy C, Zaremba W, Sutskever I, et al. Intriguing properties of neural networks[J]. arXiv preprint arXiv:1312.6199, 2013.
2. Huang X, Kroening D, Ruan W, et al. A Survey of Safety and Trustworthiness of Deep Neural Networks[J]. arXiv preprint arXiv:1812.08342, 2018.
3. Amodei D, Olah C, Steinhardt J, et al. Concrete problems in AI safety[J]. arXiv preprint arXiv:1606.06565, 2016.
4. Garcıa J, Fernández F. A comprehensive survey on safe reinforcement learning[J]. Journal of Machine Learning Research, 2015, 16(1): 1437-1480.
5. Ng A Y, Harada D, Russell S. Policy invariance under reward transformations: Theory and application to reward shaping[C]//ICML. 1999, 99: 278-287.
6. Ng A Y, Russell S J. Algorithms for inverse reinforcement learning[C]//Icml. 2000, 1: 663-670.
7. Kazak Y, Barrett C, Katz G, et al. Verifying deep-RL-driven systems[C]//Proceedings of the 2019 Workshop on Network Meets AI & ML. 2019: 83-89.